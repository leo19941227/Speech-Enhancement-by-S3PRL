dataloader:
  batch_size: 6                                         # training batch size, 12 for pre-train, 6 for cpc exp
  eval_batch_size: 12

preprocessor:
  input_channel: 0
  target_channel: 1
  
  # feat_type: complx, linear, phase, mel, mfcc
  baseline:
    feat_type: linear
    log: False
    delta: 0
    cmvn: False

runner:
  learning_rate: 4.0e-3                                 # Learning rate for opt: ['4e-3' for fine-tune, '4e-3' for regualr downstream task training]
  warmup_proportion: 0.07                               # Proportion of training to perform linear rate warmup.
  gradient_clipping: 1.0                                # Maximum gradient norm
  total_step: 20000                                            # total steps for training, a step is a batch of update
  log_step: 500                                          # log training status every this amount of training steps
  eval_step: 2000                                         # evaluate every this amount of training steps
  max_keep: 5                                           # maximum number of model ckpt to keep during training
  eval_splits: ['subtrain', 'dev', 'test']
  eval_metrics: ['stoi', 'pesq_nb', 'sisdr']
  eval_train_subset_ratio: 0.1
  eval_log_wavs_num: 5

dataset:
  dns: [
    './DNS-Challenge/',
  ]
  dns_test: [
    './DNS-Challenge/datasets/test_set/synthetic/no_reverb/',
  ]

objective:
  L1:
    log: False
  SISDR:
    {}
  WSD:
    db_interval: 30
    alpha: 0.5

model:
  Linear:
    hidden_size: 256
  LSTM:
    hidden_size: 256
    num_layers: 3
    bidirectional: False
    activation: ReLU
  Residual:
    hidden_size: 256
    num_layers: 3
    bidirectional: False
    activation: Sigmoid
    cmvn: True
